{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80ac67f7",
   "metadata": {},
   "source": [
    "1) Data Augmentation\n",
    "\n",
    "So far, we've selected a model architecture that vastly improves the model's performance, as it is designed to recognize important features in the images. The validation accuracy is still lagging behind the training accuracy, which is a sign of overfitting: the model is getting confused by things it has not seen before when it tests against the validation dataset.\n",
    "\n",
    "In order to teach our model to be more robust when looking at new data, we're going to programmatically increase the size and variance in our dataset. This is known as data augmentation, a useful technique for many deep learning applications.\n",
    "\n",
    "The increase in size gives the model more images to learn from while training. \n",
    "The increase in variance helps the model ignore unimportant features and select only the features that are truly important in classification, allowing it to generalize better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6cafdec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preparing the Data\n",
    "import tensorflow.keras as keras\n",
    "import pandas as pd\n",
    "\n",
    "# Load in our data from CSV files\n",
    "train_df = pd.read_csv(\"data/asl_data/sign_mnist_train.csv\")\n",
    "valid_df = pd.read_csv(\"data/asl_data/sign_mnist_valid.csv\")\n",
    "\n",
    "# Separate out our target values\n",
    "y_train = train_df['label']\n",
    "y_valid = valid_df['label']\n",
    "del train_df['label']\n",
    "del valid_df['label']\n",
    "\n",
    "# Separate our our image vectors\n",
    "x_train = train_df.values\n",
    "x_valid = valid_df.values\n",
    "\n",
    "# Turn our scalar targets into binary categories\n",
    "num_classes = 24\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_valid = keras.utils.to_categorical(y_valid, num_classes)\n",
    "\n",
    "# Normalize our image data\n",
    "x_train = x_train / 255\n",
    "x_valid = x_valid / 255\n",
    "\n",
    "# Reshape the image data for the convolutional network\n",
    "x_train = x_train.reshape(-1,28,28,1)\n",
    "x_valid = x_valid.reshape(-1,28,28,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28756bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model Creation\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import (\n",
    "    Dense,\n",
    "    Conv2D,\n",
    "    MaxPool2D,\n",
    "    Flatten,\n",
    "    Dropout,\n",
    "    BatchNormalization,\n",
    ")\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(75, (3, 3), strides=1, padding=\"same\", activation=\"relu\", \n",
    "                 input_shape=(28, 28, 1)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPool2D((2, 2), strides=2, padding=\"same\"))\n",
    "model.add(Conv2D(50, (3, 3), strides=1, padding=\"same\", activation=\"relu\"))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPool2D((2, 2), strides=2, padding=\"same\"))\n",
    "model.add(Conv2D(25, (3, 3), strides=1, padding=\"same\", activation=\"relu\"))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPool2D((2, 2), strides=2, padding=\"same\"))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(units=512, activation=\"relu\"))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(units=num_classes, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3c6466",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Augmentation\n",
    "#Keras comes with an image augmentation class called ImageDataGenerator\n",
    "#It accepts a series of options for augmenting your data. \n",
    "#Later in the course, we'll have you select a proper augmentation strategy\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=10,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "    zoom_range=0.1,  # Randomly zoom image\n",
    "    width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
    "    height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
    "    horizontal_flip=True,  # randomly flip images horizontally\n",
    "    vertical_flip=False, # Don't randomly flip images vertically\n",
    ") \n",
    "#RQ:we would want to flip images horizontally, but not vertically!!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a6a867",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Batch Size\n",
    "#Another benefit of the ImageDataGenerator is that it batches our data so that our model can train on a random sample.\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "batch_size = 32\n",
    "img_iter = datagen.flow(x_train, y_train, batch_size=batch_size)\n",
    "\n",
    "x, y = img_iter.next()\n",
    "fig, ax = plt.subplots(nrows=4, ncols=8)\n",
    "for i in range(batch_size):\n",
    "    image = x[i]\n",
    "    ax.flatten()[i].imshow(np.squeeze(image))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ebcbe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fitting the Data to the Generator\n",
    "#the generator must be fit on the training dataset.\n",
    "\n",
    "datagen.fit(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67747b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a9c04f",
   "metadata": {},
   "source": [
    "Training with Augmentation\n",
    "\n",
    "When using an image data generator with Keras, a model trains a bit differently: instead of just passing the x_train and y_train datasets into the model, we pass the generator in, calling the generator's flow method. This causes the images to get augmented live and in memory right before they are passed into the model for training.\n",
    "\n",
    "Generators can supply an indefinite amount of data, and when we use them to train our data, we need to explicitly set how long we want each epoch to run, or else the epoch will go on indefinitely, with the generator creating an indefinite number of augmented images to provide the model.\n",
    "\n",
    "We explicitly set how long we want each epoch to run using the steps_per_epoch named argument. Because steps * batch_size = number_of_images_trained in an epoch a common practice, that we will use here, is to set the number of steps equal to the non-augmented dataset size divided by the batch_size (which has a default value of 32).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbcc0c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(img_iter,\n",
    "          epochs=20,\n",
    "          steps_per_epoch=len(x_train)/batch_size, # Run same number of steps we would if we were not using a generator.\n",
    "          validation_data=(x_valid, y_valid))\n",
    "\n",
    "#You will notice that the validation accuracy is higher, and more consistent. \n",
    "#This means that our model is no longer overfitting in the way it was; it generalizes better, making better predictions on new data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b36e9d",
   "metadata": {},
   "source": [
    "Saving the Model\n",
    "\n",
    "Now that we have a well-trained model, we will want to deploy it to perform inference on new images.\n",
    "\n",
    "It is common, once we have a trained model that we are happy with to save it to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d4ea51",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('asl_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4743dbf7",
   "metadata": {},
   "source": [
    "2) Deploying Your Model\n",
    "\n",
    "Now that we have a well trained model, it's time to use it. In this exercise, we'll expose new images to our model and detect the correct letters of the sign language alphabet. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307b394d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the Model from disk\n",
    "from tensorflow import keras\n",
    "\n",
    "model = keras.models.load_model('asl_model')\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99a9da3",
   "metadata": {},
   "source": [
    "You'll notice that the images we have are much higher resolution than the images in our dataset. They are also in color. Remember that our images in the dataset were 28x28 pixels and grayscale. It's important to keep in mind that whenever you make predictions with a model, the input must match the shape of the data that the model was trained on. For this model, the training dataset was of the shape: (27455, 28, 28, 1). This corresponded to 27455 images of 28 by 28 pixels each with one color channel (grayscale).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e901c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "def show_image(image_path):\n",
    "    image = mpimg.imread(image_path)\n",
    "    plt.imshow(image, cmap='gray')\n",
    "show_image('data/asl_images/b.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae62eeb",
   "metadata": {},
   "source": [
    "The images in our dataset were 28x28 pixels and grayscale. We need to make sure to pass the same size and grayscale images into our method for prediction. There are a few ways to edit images with Python, but Keras has a built-in utility that works well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a75e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing import image as image_utils\n",
    "\n",
    "def load_and_scale_image(image_path):\n",
    "    image = image_utils.load_img(image_path, color_mode=\"grayscale\", target_size=(28,28))\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0003b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = load_and_scale_image('data/asl_images/b.png')\n",
    "plt.imshow(image, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e56272fd",
   "metadata": {},
   "source": [
    "Now that we have a 28x28 pixel grayscale image, we're close to being ready to pass it into our model for prediction. First we need to reshape our image to match the shape of the dataset the model was trained on. Before we can reshape, we need to convert our image into a more rudimentary format. We'll do this with a keras utility called image_to_array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2863ca77",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = image_utils.img_to_array(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3fed92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This reshape corresponds to 1 image of 28x28 pixels with one color channel\n",
    "image = image.reshape(1,28,28,1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38da8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = image / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385042e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Okay, now we're ready to predict! This is done by passing our pre-processed image into the model's predict method.\n",
    "prediction = model.predict(image)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9759f632",
   "metadata": {},
   "source": [
    "output\n",
    "[[2.5871955e-27 1.0000000e+00 0.0000000e+00 1.4255421e-32 4.0587904e-18\n",
    "  1.2571522e-27 1.0431842e-31 0.0000000e+00 3.6695539e-18 2.2435437e-36\n",
    "  1.6701662e-32 7.7334602e-35 1.2264817e-33 0.0000000e+00 5.9859541e-33\n",
    "  4.0133789e-37 1.7237054e-36 0.0000000e+00 0.0000000e+00 1.8616249e-20\n",
    "  0.0000000e+00 2.1368446e-16 1.2500648e-20 7.1852069e-35]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4479f5c2",
   "metadata": {},
   "source": [
    "Understanding the Prediction\n",
    "\n",
    "The predictions are in the format of a 24 length array. Though it looks a bit different, this is the same format as our \"binarized\" categorical arrays from y_train and y_test. Each element of the array is a probability between 0 and 1, representing the confidence for each category. Let's make it a little more readable. We can start by finding which element of the array represents the highest probability. This can be done easily with the numpy library and the argmax function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a228b633",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.argmax(prediction)\n",
    "#output 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3a371b",
   "metadata": {},
   "source": [
    "Each element of the prediction array represents a possible letter in the sign language alphabet. Remember that j and z are not options because they involve moving the hand, and we're only dealing with still photos. Let's create a mapping between the index of the predictions array, and the corresponding letter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0aad19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alphabet does not contain j or z because they require movement\n",
    "alphabet = \"abcdefghiklmnopqrstuvwxy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79f08d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can now pass in our prediction index to find the corresponding letter.\n",
    "alphabet[np.argmax(prediction)]\n",
    "#output 'b'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc8dee7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
